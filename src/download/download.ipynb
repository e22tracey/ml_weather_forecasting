{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benson = EGUB/03658\n",
    "\n",
    "51.6201, -1.0983"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import cfgrib\n",
    "import numpy as np\n",
    "import requests\n",
    "import xarray as xr\n",
    "from dask.distributed import wait, Client as DaskClient\n",
    "# from ecmwf.opendata import Client as ECMWFClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_client = DaskClient(n_workers=5, threads_per_worker=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamical - GFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_gfs = xr.open_zarr(\"https://data.dynamical.org/noaa/gfs/forecast/latest.zarr?email=email@email.com\")\n",
    "ds_gfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variable_list = [\n",
    "#     \"pressure_reduced_to_mean_sea_level\",\n",
    "#     \"relative_humidity_2m\", \n",
    "#     \"temperature_2m\", \n",
    "#     \"total_cloud_cover_atmosphere\", \n",
    "#     \"wind_u_10m\",\n",
    "#     \"wind_v_10m\"\n",
    "#     ]\n",
    "\n",
    "variable_list = [\n",
    "    \"relative_humidity_2m\", \n",
    "    \"temperature_2m\", \n",
    "    \"total_cloud_cover_atmosphere\", \n",
    "    \"wind_u_10m\",\n",
    "    \"wind_v_10m\",\n",
    "]\n",
    "\n",
    "ds_gfs_subset = ds_gfs[variable_list].sel(\n",
    "    latitude=slice(52, 51.25),\n",
    "    longitude=slice(-1.5, -0.75),\n",
    "    lead_time=slice(np.timedelta64(0, 'h'), np.timedelta64(119, 'h')),\n",
    "    init_time=slice(\"2024-10-24T00:00:00.000000000\", \"2025-10-24T18:00:00.000000000\"),\n",
    "    )\n",
    "ds_gfs_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_gfs_subset.load()\n",
    "ds_gfs_subset.to_zarr(\"data/gfs_benson_1yr.zarr\", mode=\"a\")\n",
    "\n",
    "ds_gfs.close()\n",
    "ds_gfs_subset.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_zarr(\"/Users/Shared/scratch/ml_weather_forecasting/gfs_benson_1yr.zarr\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Occassionally the download may miss some data, we can check for nan values and then return dates with missing data to re-download\n",
    "missing_dates_list = set(np.where(np.isnan(ds.wind_v_10m))[0].tolist())\n",
    "\n",
    "missing_dates_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ECMWF - IFS\n",
    "\n",
    "Data starts 2023-01-18\n",
    "\n",
    "There have been some changes to ECMWF's file structures over the last couple of years - these are all of the combinations I've found so far. There are also dates where AIFS is missing, as they were experimenting with it.\n",
    "\n",
    "- url1 to 20240131 00z = [yyyymmdd]/[run]z/0p4-beta/oper/[yyyymmdd][run]0000-[step]h-oper-fc.grib2\n",
    "\n",
    "`url = f\"{url_base}/{date_str}/{run}z/0p4-beta/oper/{date_str}{run}0000-{step}h-oper-fc.grib2\"`\n",
    "\n",
    "- url2 to 20240228 00z = [yyyymmdd]/[run]z/0p25/oper/[yyyymmdd][run]0000-[step]h-oper-fc.grib2\n",
    "- url3 to 20250123 18z = [yyyymmdd]/[run]z/[ifs|aifs]/0p25/oper/[yyyymmdd][run]0000-[step]h-oper-fc.grib2\n",
    "- url4 to 20250209 06z = [yyyymmdd]/[run]z/[ifs]/0p25/oper/[yyyymmdd][run]0000-[step]h-oper-fc.grib2    * missing AIFS *\n",
    "- url5 to 20250225 00z = [yyyymmdd]/[run]z/[ifs|aifs-single]/0p25/[experimental]/oper/[yyyymmdd][run]0000-[step]h-oper-fc.grib2     * Experimental for AIFS *\n",
    "- url6 to 20250227 18z =[yyyymmdd]/[run]z/[ifs]/0p25/oper/[yyyymmdd][run]0000-[step]h-oper-fc.grib2    * missing AIFS *\n",
    "- url7 to now = [yyyymmdd]/[run]z/[ifs|aifs-single]/0p25/oper/[yyyymmdd][run]0000-[step]h-oper-fc.grib2\n",
    "\n",
    "`url = f\"{url_base}/{date_str}/{run}z/aifs-single/0p25/oper/{date_str}{run}0000-{step}h-oper-fc.grib2\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_str = \"20241024\"\n",
    "run = \"00\"\n",
    "step = \"0\"\n",
    "base_url = \"https://storage.googleapis.com/ecmwf-open-data\"\n",
    "response = requests.get(f\"{base_url}/{date_str}/{run}z/ifs/0p25/oper/{date_str}{run}0000-{step}h-oper-fc.grib2\")\n",
    "\n",
    "grib_file = \"test_grib.grib2\"\n",
    "if response.status_code == 200:\n",
    "    with open(grib_file, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "else:\n",
    "    print(f\"Couldn't download {date_str} {run}z {step}h - Status code: {response.status_code}\")\n",
    "\n",
    "ds_list = cfgrib.open_datasets(grib_file)\n",
    "ds_list_final = []\n",
    "vars = ['t2m', 'd2m', 'msl', 'u10', 'v10', 'tcc']\n",
    "for ds_n in ds_list:\n",
    "    var_names = [var for var in vars if var in ds_n.data_vars]\n",
    "    if len(var_names) > 0:\n",
    "        if 'heightAboveGround' in ds_n.coords:\n",
    "            ds_n = ds_n.drop_vars('heightAboveGround')\n",
    "        ds_list_final.append(ds_n[var_names])\n",
    "\n",
    "ds = xr.merge(ds_list_final, compat=\"no_conflicts\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_url_process_data(url, grib_file_path, nc_file_path, date_str, run, step):\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        with open(grib_file_path, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"Downloaded {date_str} {run}z {step}h\")\n",
    "    else:\n",
    "        print(f\"Couldn't download {date_str} {run}z {step}h - Status code: {response.status_code}\")\n",
    "\n",
    "    ds_list = cfgrib.open_datasets(grib_file_path)\n",
    "    ds_list_final = []\n",
    "    vars = ['t2m', 'd2m', 'msl', 'u10', 'v10', 'tcc']\n",
    "    for ds_n in ds_list:\n",
    "        var_names = [var for var in vars if var in ds_n.data_vars]\n",
    "        if len(var_names) > 0:\n",
    "            if 'heightAboveGround' in ds_n.coords:\n",
    "                ds_n = ds_n.drop_vars('heightAboveGround')\n",
    "            ds_list_final.append(ds_n[var_names])\n",
    "\n",
    "    ds = xr.merge(ds_list_final, compat=\"no_conflicts\")\n",
    "    ds = ds.sel(latitude=slice(52, 51.25), longitude=slice(-1.5, -0.75))\n",
    "    ds.to_netcdf(nc_file_path)\n",
    "    print(f\"Saved {nc_file_path}\")\n",
    "\n",
    "    ds.close()\n",
    "    for f in glob.glob(f\"{grib_file_path}*\"):\n",
    "        os.remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_picker(url_base: str, model: str, date: dt.datetime, run: str, step: int) -> str|None:\n",
    "    date_str = date.strftime(\"%Y%m%d\")\n",
    "\n",
    "    if model == \"aifs\":\n",
    "        model = \"aifs-single\"\n",
    "\n",
    "    url_dict = {\n",
    "        \"url1\":f\"{url_base}/{date_str}/{run}z/0p4-beta/oper/{date_str}{run}0000-{step}h-oper-fc.grib2\",\n",
    "        \"url2\":f\"{url_base}/{date_str}/{run}z/0p25/oper/{date_str}{run}0000-{step}h-oper-fc.grib2\",\n",
    "        \"url3\":f\"{url_base}/{date_str}/{run}z/{model}/0p25/oper/{date_str}{run}0000-{step}h-oper-fc.grib2\",\n",
    "        }\n",
    "    \n",
    "    if dt.datetime(2023, 1, 18) <= date <= dt.datetime(2024, 1, 31):\n",
    "        if date == dt.datetime(2024, 1, 31) and (run in [\"06\", \"12\", \"18\"]):\n",
    "            return url_dict[\"url2\"]\n",
    "        else:\n",
    "            return url_dict[\"url1\"]\n",
    "    elif dt.datetime(2024, 2, 1) <= date <= dt.datetime(2024, 2, 28):\n",
    "        if date == dt.datetime(2024, 2, 28) and (run in [\"06\", \"12\", \"18\"]):\n",
    "            return url_dict[\"url3\"]\n",
    "        else:\n",
    "            return url_dict[\"url2\"]\n",
    "    elif dt.datetime(2024, 3, 1) <= date:\n",
    "        return url_dict[\"url3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url_base = \"https://storage.googleapis.com/ecmwf-open-data\"\n",
    "url_base = \"https://ecmwf-forecasts.s3.eu-central-1.amazonaws.com\"\n",
    "model = \"ifs\"\n",
    "\n",
    "# Create a list of dates to be downloaded\n",
    "start_dt = dt.datetime(2024, 10, 24)\n",
    "end_dt = dt.datetime(2025, 10, 24)\n",
    "\n",
    "if end_dt > dt.datetime.now():\n",
    "    print(\"End date is in the future, setting to today's date\")\n",
    "    end_dt = dt.datetime.now()\n",
    "if model == \"aifs\" and start_dt < dt.datetime(2025, 2, 28):\n",
    "    raise Exception(\"AIFS data not available before 2025-02-28\")\n",
    "\n",
    "delta = dt.timedelta(days=1)\n",
    "\n",
    "dates = []\n",
    "while start_dt <= end_dt:\n",
    "    dates.append(start_dt)\n",
    "    start_dt += delta\n",
    "\n",
    "# Choose what runs to be downloaded\n",
    "runs = [\"00\", \"12\"]\n",
    "\n",
    "# Choose what steps to be downloaded\n",
    "# IFS:\n",
    "# - For times 00z &12z: 0 to 144 by 3, 150 to 360 by 6.\n",
    "# - For times 06z & 18z: 0 to 144 by 3.\n",
    "#\n",
    "# AIFS:\n",
    "# - 6 hourly steps to 360 (15 days)\n",
    "\n",
    "if model == \"ifs\":\n",
    "    steps = list(range(0, 73, 3))\n",
    "elif model == \"aifs\":\n",
    "    steps = list(range(0, 121, 6))\n",
    "\n",
    "# Setup loops over dates, runs, steps\n",
    "futures = []\n",
    "ds_list = []\n",
    "for date in dates:\n",
    "    date_str = date.strftime(\"%Y%m%d\")\n",
    "\n",
    "    for run in runs:\n",
    "        date_folder = f\"data/ecmwf_{model}/{date_str}_{run}z\"\n",
    "        os.makedirs(date_folder, exist_ok=True)\n",
    "\n",
    "        for step in steps:\n",
    "            # Create URL based on the date to be downloaded, using the formats found in the data stores\n",
    "            url = url_picker(url_base, model, date, run, step)\n",
    "\n",
    "            grib_file_path = os.path.join(date_folder, f\"{model}_{date_str}_{run}z_{step}h.grib2\")\n",
    "            nc_file_path = os.path.join(date_folder, f\"{model}_{date_str}_{run}z_{step}h.nc\")\n",
    "\n",
    "            future = dask_client.submit(request_url_process_data, url, grib_file_path, nc_file_path, date_str, run, step)\n",
    "            futures.append(future)\n",
    "\n",
    "wait(futures)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
